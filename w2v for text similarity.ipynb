{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec +余弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_file = 'E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt'\n",
    "model_file = 'E:/litao/Chatbots/data/word2vec/news_12g_baidubaike_20g_novel_90g_embedding_64.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_file, binary=True,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import jieba\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "from pyhanlp import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_words = []\n",
    "f = open('E:/litao/Chatbots/data/ChineseStopWords.txt','r',encoding='ISO-8859-1')\n",
    "for w in f.readlines():\n",
    "    stop_words.append(w)\n",
    "\n",
    "\n",
    "def sentence_vector(s):\n",
    "       # words = HanLP.extractKeyword(s, 10)\n",
    "        words = jieba.lcut(s)\n",
    "        words = ' '.join(words).split()\n",
    "        #print(words)\n",
    "        v = np.zeros(64)\n",
    "        clean_w = []\n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                if word in model:\n",
    "                    v += model[word]\n",
    "                    clean_w.append(word)\n",
    "        \n",
    "       # print(len(clean_w))\n",
    "        if len(clean_w)>0 and v.any() >0:            \n",
    "                v /= len(clean_w)\n",
    "                return v\n",
    "        else:\n",
    "            return v\n",
    "def word2vec(file_path):\n",
    "    f = open(file_path,'r', encoding='utf-8')\n",
    "    wv = []\n",
    "    wv_dict = {}\n",
    "    for  i ,line in enumerate(f.readlines()):\n",
    "        line = json.loads(line)\n",
    "        question = line['title']\n",
    "        answer = line['answer']      \n",
    "        v = sentence_vector(question)\n",
    "        #if v.any() != 0:\n",
    "        wv_dict[i] = (question,answer)\n",
    "        wv.append(v)\n",
    "    return wv,wv_dict\n",
    "\n",
    "file_path = 'E:/litao/Chatbots/data/baike_qa/baike_qa_train.json'\n",
    "wv,wv_dict = word2vec(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = '松本面板可以配什么品牌的超五类模块??'\n",
    "sent2 = '在线等“神通卡”的问题神通卡绑定游戏ID，能使用枪15天过了15'\n",
    "v1 = sentence_vector(sent1)\n",
    "v2= sentence_vector(sent2)\n",
    "sim= np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import *\n",
    "def vector_similarity(v1,wv,wv_dict):\n",
    "    sims =  []\n",
    "    sims_dict ={}\n",
    "    for i,v2 in enumerate(wv):\n",
    "         if v1.any() != 0 and v2.any() != 0:\n",
    "                sim = np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "                #print(sim)\n",
    "                #print(v2)\n",
    "                sims.append(sim)\n",
    "                sims_dict[sim] = i\n",
    "                \n",
    "                \n",
    "    sims.sort(reverse=True)\n",
    "    #print(sims[0])\n",
    "    #print(sims_dict)\n",
    "    top_n = 3\n",
    "    for i in range(top_n):\n",
    "        print (sims[i])\n",
    "        print( wv_dict[sims_dict[sims[i]]])\n",
    "\n",
    "begin_time = time()  \n",
    "\n",
    "userinput1 = '下雨怎么办？'\n",
    "userinput2 = '胃痛怎么办？'\n",
    "userinput3 = '如何学好数学？ '\n",
    "v1 = sentence_vector(userinput1)\n",
    "v2 = sentence_vector(userinput2)\n",
    "v3 = sentence_vector(userinput3)\n",
    "print('---------')\n",
    "vector_similarity(v1,wv,wv_dict)\n",
    "print('---------')\n",
    "vector_similarity(v2,wv,wv_dict)\n",
    "print('---------')\n",
    "vector_similarity(v3,wv,wv_dict)\n",
    "print('---------')\n",
    "\n",
    "end_time=time()\n",
    "run_time = (end_time-begin_time)/3\n",
    "print ('Word2vec相似度运行时间：',run_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
