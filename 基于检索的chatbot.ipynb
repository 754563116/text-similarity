{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>天下雨怎么办\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-23 10:51:59,093 : INFO : loading projection weights from E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt\n",
      "2019-08-23 10:52:07,350 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:07,601 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:07,935 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:07,969 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:09,071 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:09,345 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:09,710 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:11,165 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:11,395 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:11,779 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:11,941 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:12,806 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:13,199 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:13,449 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:13,654 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:14,296 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:14,548 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:15,016 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:15,047 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:15,312 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:15,400 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:15,850 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:16,458 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:17,275 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:17,514 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:17,864 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:18,928 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:19,131 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-23 10:52:19,223 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:19,235 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:19,512 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:20,535 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:20,945 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:20,960 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:21,816 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:21,911 : WARNING : duplicate word '..................................................................................................' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:21,970 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:22,536 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:23,089 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:23,311 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:25,699 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:25,893 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:26,212 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:26,383 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:26,868 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:29,556 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:31,632 : WARNING : duplicate word '==================================================================================================' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:32,287 : WARNING : duplicate word '==================================================================================================' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:32,586 : WARNING : duplicate word '==================================================================================================' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:52:33,845 : WARNING : duplicate word '**************************************************************************************************' in E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt, ignoring all but first\n",
      "2019-08-23 10:53:46,712 : INFO : duplicate words detected, shrinking matrix size from 399999 to 399949\n",
      "2019-08-23 10:53:46,713 : INFO : loaded (399949, 300) matrix from E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f9d05e28f0e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;31m#print(userInput)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'E:/litao/Chatbots/data/baike_qa/baike_qa_valid.json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0msim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserInput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msim\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f9d05e28f0e3>\u001b[0m in \u001b[0;36mQA\u001b[1;34m(file_path, user_question)\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mq_w\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                          \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m                          \u001b[0mq_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muser_q_w\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "#完全基于检索的chatbot\n",
    "#劣势：需要每次都去计算question和语料中的所有问题的相似度\n",
    "import jieba\n",
    "import gensim\n",
    "import json\n",
    "import  numpy as np\n",
    "import random\n",
    "\n",
    "#计算相似度\n",
    "#通过词向量的平均相似度\n",
    "def similarity(user_q_vec,q_vec):\n",
    "    max_top = []\n",
    "    cosine_sim = 0\n",
    "    for u_vec in user_q_vec:\n",
    "            for vec in q_vec:\n",
    "                vec = np.mat(vec)\n",
    "                u_vec = np.mat(u_vec)\n",
    "                num = float(vec * u_vec.T)\n",
    "                denom = np.linalg.norm(vec) * np.linalg.norm(u_vec)\n",
    "                cos = num / denom\n",
    "                sim = 0.5 + 0.5 * cos\n",
    "                cosine_sim += sim\n",
    "            #返回平均相似度\n",
    "   # print(len(user_q_vec))\n",
    "    #print(len(q_vec))\n",
    "    return int(cosine_sim)/(len(user_q_vec)*len(q_vec))\n",
    "\n",
    "\n",
    "def QA(user_question,vec_path):\n",
    "    user_q_list = jieba.cut(user_question)\n",
    "    user_q_w = ' '.join(user_q_list).split()\n",
    "    user_q_vec = []\n",
    "    dict_sim = {}\n",
    "    for w in user_q_w:\n",
    "         if w in model:\n",
    "                user_q_vec.append(model[w])\n",
    "    f = open(vec_path,'r',encoding='utf-8')\n",
    "    for q_vec in f.readlines():\n",
    "        if len(user_q_vec) != 0 and len(q_vec) != 0:\n",
    "                   sims.append(similarity(user_q_vec,q_vec[:-1]))\n",
    "                   dict_sim[similarity(user_q_vec,q_vec)] = q_vec[-1]\n",
    "    sims = sims.sort()\n",
    "    top_n = 3\n",
    "    for i in range(top_n):\n",
    "        return sims[i],sims_dicts[sims[i]] \n",
    "    \n",
    "def dialog():\n",
    "    greeting_list1 = ['你好','您好']\n",
    "    greeting_list2 = ['你好，我是百科问答机器人小涛，有什么可以帮到您的吗？(*￣︶￣)','您好，我是百科问答机器人小涛，有什么可以帮到您的吗？(*￣︶￣)']#开始对话，问候意图\n",
    "   # bot_info = ['我是']\n",
    "    while True:\n",
    "            userInput = input(\">>>\")\n",
    "            userInput_list = jieba.cut(userInput)\n",
    "            userInput_word = ' '.join(userInput_list).split()\n",
    "            if  not set(userInput_word).isdisjoint(greeting_list1):\n",
    "                print(random.choice(greeting_list2))\n",
    "            elif  userInput == '你爸爸是谁':\n",
    "                  print('我爸爸是真气工程师，(*￣︶￣)')\n",
    "            elif  userInput == '你妈妈是谁':\n",
    "                  print('我妈妈是真气工程师，(*￣︶￣)')\n",
    "            elif  userInput == '你叫什么':\n",
    "                  print('我的名字是小涛，(*￣︶￣)')\n",
    "            elif  userInput == '你多少岁了':\n",
    "                  print('我今年8岁了哦,(*￣︶￣)')\n",
    "            elif  userInput == '你家在哪':\n",
    "                  print('我家在杭州哦,(*￣︶￣)')\n",
    "            #结束意图\n",
    "            elif userInput == '再见':\n",
    "                print('很高兴为您解答，欢迎再次询问,(*￣︶￣)')\n",
    "                break\n",
    "            #问问题意图还有待优化，用分类来分场景\n",
    "            elif len(userInput) != 0:\n",
    "                vec_path = 'E:/litao/Chatbots/data/baike_qa/baike_valid_vec'\n",
    "                sim,answer = QA(userInput,vec_path)\n",
    "                if sim>=0.2:\n",
    "                    print(answer) \n",
    "                else:\n",
    "                    print('对不起，我不懂您的问题！')\n",
    "            #无意图\n",
    "            else:\n",
    "                print(\"对不起，我不明白您讲得是什么！\")\n",
    "if __name__=='__main__':\n",
    "        dialog()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用gensim提供的tfidf计算相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1425170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-26 09:39:50,583 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-08-26 09:39:50,798 : INFO : adding document #10000 to Dictionary(18135 unique tokens: ['上', '为什么', '人站', '在', '地球']...)\n",
      "2019-08-26 09:39:51,020 : INFO : adding document #20000 to Dictionary(25902 unique tokens: ['上', '为什么', '人站', '在', '地球']...)\n",
      "2019-08-26 09:39:51,236 : INFO : adding document #30000 to Dictionary(30980 unique tokens: ['上', '为什么', '人站', '在', '地球']...)\n",
      "2019-08-26 09:39:51,452 : INFO : adding document #40000 to Dictionary(34770 unique tokens: ['上', '为什么', '人站', '在', '地球']...)\n",
      "2019-08-26 09:39:51,687 : INFO : adding document #50000 to Dictionary(37500 unique tokens: ['上', '为什么', '人站', '在', '地球']...)\n",
      "2019-08-26 09:39:51,933 : INFO : adding document #60000 to Dictionary(39532 unique tokens: ['上', '为什么', '人站', '在', '地球']...)\n",
      "2019-08-26 09:39:52,172 : INFO : adding document #70000 to Dictionary(41080 unique tokens: ['上', '为什么', '人站', '在', '地球']...)\n",
      "2019-08-26 09:39:52,392 : INFO : adding document #80000 to Dictionary(42163 unique tokens: ['上', '为什么', '人站', '在', '地球']...)\n",
      "2019-08-26 09:39:52,612 : INFO : adding document #90000 to Dictionary(42909 unique tokens: ['上', '为什么', '人站', '在', '地球']...)\n",
      "2019-08-26 09:39:52,831 : INFO : built Dictionary(43420 unique tokens: ['上', '为什么', '人站', '在', '地球']...) from 100000 documents (total 1532038 corpus positions)\n",
      "2019-08-26 09:39:54,077 : INFO : collecting document frequencies\n",
      "2019-08-26 09:39:54,078 : INFO : PROGRESS: processing document #0\n",
      "2019-08-26 09:39:54,104 : INFO : PROGRESS: processing document #10000\n",
      "2019-08-26 09:39:54,133 : INFO : PROGRESS: processing document #20000\n",
      "2019-08-26 09:39:54,161 : INFO : PROGRESS: processing document #30000\n",
      "2019-08-26 09:39:54,190 : INFO : PROGRESS: processing document #40000\n",
      "2019-08-26 09:39:54,219 : INFO : PROGRESS: processing document #50000\n",
      "2019-08-26 09:39:54,248 : INFO : PROGRESS: processing document #60000\n",
      "2019-08-26 09:39:54,277 : INFO : PROGRESS: processing document #70000\n",
      "2019-08-26 09:39:54,307 : INFO : PROGRESS: processing document #80000\n",
      "2019-08-26 09:39:54,336 : INFO : PROGRESS: processing document #90000\n",
      "2019-08-26 09:39:54,366 : INFO : calculating IDF weights for 100000 documents and 43420 features (1320058 matrix non-zeros)\n",
      "2019-08-26 09:39:54,467 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2019-08-26 09:40:00,991 : INFO : creating matrix with 100000 documents and 43420 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43939, 0.77923834)\n",
      "('如何锻炼身体 ', '我建议您连练散打 \\r\\n中国武术散打是世界上最厉害的格斗对抗性（打斗）的功夫，学是很简单的你找个体校武术俱乐部都可以去学， \\r\\n记住最简单的就是最难的，最难的也就是最简单的，你不要想怎么难学，只要在教练的指导下你认识学习就可以了 \\r\\n至于想练牛比吗，有的一年就可以，有的3、5年也不行 \\r\\n要看你怎么练了，教练怎么规定你就怎么练这样一年就能练的牛，最主要的你训练不要逃课， \\r\\n好就这样祝你学业有成，早日成为散打高')\n"
     ]
    }
   ],
   "source": [
    " # import warnings\n",
    " # warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import logging\n",
    "from gensim import corpora, models, similarities\n",
    "import jieba\n",
    "import torch\n",
    "from collections import defaultdict  \n",
    "import json\n",
    "\n",
    "#计算相似度，并返回对应答案\n",
    "def similarity(datapath, query):\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    with open(datapath, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()\n",
    "                    texts = []\n",
    "                    sents = {} \n",
    "                    print(len(lines))\n",
    "                    for i ,line in enumerate(lines[:100000]):\n",
    "                        line = json.loads(line)\n",
    "                      \n",
    "                        sent = line['title']\n",
    "                        sent1 = line['answer'] \n",
    "                        q_list = jieba.cut(sent)\n",
    "                        texts.append(' '.join(q_list).split())\n",
    "                        sents[i] = (sent,sent1)\n",
    "                   \n",
    "     #2.计算词频  \n",
    "    frequency = defaultdict(int) #构建一个字典对象  \n",
    "    #遍历分词后的结果集，计算每个词出现的频率  \n",
    "    for text in texts:  \n",
    "        for token in text:  \n",
    "            frequency[token]+=1  \n",
    "    #选择频率大于1的词  \n",
    "    Corp=[[token for token in text if frequency[token]>1] for text in texts]  \n",
    " \n",
    "    dictionary = corpora.Dictionary(Corp)\n",
    "    corpus = [dictionary.doc2bow(text) for text in Corp]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    #处理用户问题\n",
    "    query_list = jieba.cut(query)\n",
    "    query_w = ' '.join(query_list).split()\n",
    "    vec_bow = dictionary.doc2bow(query_w)\n",
    "    vec_tfidf = tfidf[vec_bow]\n",
    "    index = similarities.MatrixSimilarity(corpus_tfidf)\n",
    "   # print(index)\n",
    "    sims = index[vec_tfidf]\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    \n",
    "    \n",
    "    print(sims[0])\n",
    "    print(sents[sims[0][0]])\n",
    "\n",
    "    #开始保存\n",
    "    \n",
    "    import pickle\n",
    "    with open('E:/litao/Chatbots/data/baike_qa/baike_train_temp.pkl', 'wb') as outp:\n",
    "        pickle.dump( tfidf, outp)\n",
    "        pickle.dump(corpus_tfidf, outp)\n",
    "        pickle.dump(sents, outp)\n",
    "        pickle.dump(dictionary,outp)\n",
    "    \n",
    "datapath = 'E:/litao/Chatbots/data/baike_qa/baike_qa_train.json'\n",
    "query = '家里锻炼身体方法'\n",
    "similarity(datapath, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载保存好的训练temp.pkl\n",
    "import pickle\n",
    "with open('E:/litao/Chatbots/data/baike_qa/baike_train_temp.pkl', 'rb') as inp:\n",
    "        tfidf = pickle.load(inp)\n",
    "        corpus_tfidf = pickle.load(inp)\n",
    "        sents = pickle.load(inp)\n",
    "        dictionary = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用保存的模型和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70945,\n",
       " ('请翻译我们正要出发这时天下雨了 ', 'We were about to start out when it began to rain.'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict  \n",
    "import jieba\n",
    "import pickle\n",
    "import random\n",
    "#计算相似度并返回答案\n",
    "def similarity(query): \n",
    "    #处理用户问题   \n",
    "    query_list = jieba.cut(query)\n",
    "    query_w = ' '.join(query_list).split()\n",
    "    vec_bow = dictionary.doc2bow(query_w)\n",
    "    vec_tfidf = tfidf[vec_bow]\n",
    "    index = similarities.MatrixSimilarity(corpus_tfidf)\n",
    "    sims = index[vec_tfidf]\n",
    "    #对sim排序\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    top_n = 3\n",
    "    for i in range(top_n):\n",
    "      return sims[0][i] ,sents[sims[0][i]]\n",
    "\"\"\"\n",
    "def dialog():\n",
    "    greeting_list1 = ['你好','您好']\n",
    "    greeting_list2 = ['你好，我是百科问答机器人小涛，有什么可以帮到您的吗？(*￣︶￣)','您好，我是百科问答机器人小涛，有什么可以帮到您的吗？(*￣︶￣)']#开始对话，问候意图\n",
    "   # bot_info = ['我是']\n",
    "    while True:\n",
    "            userInput = input(\">>>\")\n",
    "            userInput_list = jieba.cut(userInput)\n",
    "            userInput_word = ' '.join(userInput_list).split()\n",
    "            if  not set(userInput_word).isdisjoint(greeting_list1):\n",
    "                print(random.choice(greeting_list2))\n",
    "            elif  userInput == '你爸爸是谁':\n",
    "                  print('我爸爸是真气工程师，(*￣︶￣)')\n",
    "            elif  userInput == '你妈妈是谁':\n",
    "                  print('我妈妈是真气工程师，(*￣︶￣)')\n",
    "            elif  userInput == '你叫什么':\n",
    "                  print('我的名字是小涛，(*￣︶￣)')\n",
    "            elif  userInput == '你多少岁了':\n",
    "                  print('我今年8岁了哦,(*￣︶￣)')\n",
    "            elif  userInput == '你家在哪':\n",
    "                  print('我家在杭州哦,(*￣︶￣)')\n",
    "            #结束意图\n",
    "            elif userInput == '再见':\n",
    "                print('很高兴为您解答，欢迎再次询问,(*￣︶￣)')\n",
    "                break\n",
    "            #问问题意图还有待优化，用分类来分场景\n",
    "            elif len(userInput) != 0:\n",
    "                sim,answer = similarity(userInput)\n",
    "                if sim>=0.2:\n",
    "                    print(answer) \n",
    "                else:\n",
    "                    print('对不起，我不懂您的问题！')\n",
    "            #无意图\n",
    "            else:\n",
    "                print(\"对不起，我不明白您讲得是什么！\")\n",
    "if __name__=='__main__':\n",
    "        dialog()\n",
    " \"\"\"  \n",
    "similarity('天下雨怎么办？')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec +余弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_file = 'E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt'\n",
    "model_file = 'E:/litao/Chatbots/data/word2vec/news_12g_baidubaike_20g_novel_90g_embedding_64.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_file, binary=True,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "import jieba\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "from pyhanlp import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_words = []\n",
    "f = open('E:/litao/Chatbots/data/ChineseStopWords.txt','r',encoding='ISO-8859-1')\n",
    "for w in f.readlines():\n",
    "    stop_words.append(w)\n",
    "\n",
    "\n",
    "def sentence_vector(s):\n",
    "       # words = HanLP.extractKeyword(s, 10)\n",
    "        words = jieba.lcut(s)\n",
    "        words = ' '.join(words).split()\n",
    "        #print(words)\n",
    "        v = np.zeros(64)\n",
    "        clean_w = []\n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                if word in model:\n",
    "                    v += model[word]\n",
    "                    clean_w.append(word)\n",
    "        \n",
    "       # print(len(clean_w))\n",
    "        if len(clean_w)>0 and v.any() >0:            \n",
    "                v /= len(clean_w)\n",
    "                return v\n",
    "        else:\n",
    "            return v\n",
    "def word2vec(file_path):\n",
    "    f = open(file_path,'r', encoding='utf-8')\n",
    "    wv = []\n",
    "    wv_dict = {}\n",
    "    for  i ,line in enumerate(f.readlines()):\n",
    "        line = json.loads(line)\n",
    "        question = line['title']\n",
    "        answer = line['answer']      \n",
    "        v = sentence_vector(question)\n",
    "        #if v.any() != 0:\n",
    "        wv_dict[i] = (question,answer)\n",
    "        wv.append(v)\n",
    "    return wv,wv_dict\n",
    "\n",
    "file_path = 'E:/litao/Chatbots/data/baike_qa/baike_qa_train.json'\n",
    "wv,wv_dict = word2vec(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7540475847136622\n"
     ]
    }
   ],
   "source": [
    "sent1 = '松本面板可以配什么品牌的超五类模块??'\n",
    "sent2 = '在线等“神通卡”的问题神通卡绑定游戏ID，能使用枪15天过了15'\n",
    "v1 = sentence_vector(sent1)\n",
    "v2= sentence_vector(sent2)\n",
    "sim= np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('人站在地球上为什么没有头朝下的感觉 ', '地球上重力作用一直是指向球心的，因此\\r\\n只要头远离球心，人们就回感到头朝上。')\n"
     ]
    }
   ],
   "source": [
    "print(wv_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "0.9347828494332172\n",
      "('下雨天怎么办 ', '做平时该做的是啊,如果心情不好,可以听音乐啊.还会让自己的心静下来,再欣赏雨也不错啊.')\n",
      "0.9043319746468587\n",
      "('太热怎么办 ', '病情分析：\\n您好，您孩子的病是由于着凉加上孩子的体质不好引起的，得积极治疗，因为小孩免疫系统不稳定，病情变化快。\\n指导意见：\\n长时间的咳嗽容易对肺部造成损害，如果抗生素治疗无效的话，可以给孩子吃点中药，中药对咳嗽有很好的疗效。我这里有个经验方对长期咳嗽有很好的疗效，介绍给您，希望您能用的上。\\n鲜芦根 30g 银花 9g 菊花 6g 杏仁9g 前胡9g\\n枇杷叶9g 天竺黄 9g 象贝母 9g 兰根 9g苏子6g\\n水煎服 一剂煎2次 一日口服2次，每次50ml\\n3-7天会见效！\\n')\n",
      "0.8915026851865874\n",
      "('中暑怎么办 ', '发现自己和其他人有先兆中暑和轻症中暑表现时，首先要做的是迅速撤离引起中暑的高温环境，选择阴凉通风的地方休息；并多饮用一些含盐分的清凉饮料。还可以在额部、颞部涂抹清凉油、风油精等，或服用人丹、十滴水、藿香正气水等中药。如果出现血压降低、虚脱时应立即平卧，及时上医院静脉滴注盐水。对于重症中暑者除了立即把中暑者从高温环境中转移至阴凉通风处外，还应该迅速将其送至医院，同时采取综合措施进行救治。若远离医院,应将病人脱离高温环境，用湿床单或湿衣服包裹病人并给强力风扇，以增加蒸发散热.在等待转运期间，可将病人浸泡于湖泊或河流，或甚至用雪或冰冷却，也是一种好办法。')\n",
      "---------\n",
      "0.9999999999999998\n",
      "('胃痛怎么办？ ', '先做个胃镜检查一下是什么问题，才好对症下药')\n",
      "0.9772506871573414\n",
      "('胃疼怎么办？ ', '是不是吃坏了东西，或是平时吃饭不规律导致的。有胃病的就更加了，要注意饮食规律，少吃辛辣刺激的食物，多吃点养胃的东西。像早餐可以吃面条或煮粥喝对胃都比较好。实在很痛，最好是到医院去做一下检查')\n",
      "0.9725520132099601\n",
      "('牙痛怎么办？ ', '牙疼即牙痛，是牙齿疾病最常见的症状之一。很多牙病能引起牙痛，    \\r常见的有龋齿、急性牙髓炎、慢性牙髓炎、牙周炎、牙龈炎等。此外，某些神经系统疾病，如三叉神经痛、周围性面神经炎等；身体的某些慢性疾病，如高血压病患者牙髓充血、糖尿病患者牙髓血管发炎坏死等都可引起牙痛。目前，止牙疼的方法有很多种。但是，行之有效的办法还是有限。“牙痛不是病，疼起来真要命。”这种说法流传民间好久，其实，牙痛就是你牙齿有病的外在反应。中医认为牙痛是人体 实、虚火上升引起的。实火引起的，痛程短，疼痛厉害；虚火 引起的，痛程长，疼痛绵绵，牙齿松动。\\r一、中医治疗牙痛\\r\\u3000\\u3000中医治疗牙疼我我们中国人自己发明的治疗牙疼的方法，在悠久的中国    \\r历史上曾出现过许多优秀的中医治疗牙疼的专家，相信我们这个的传统医术治疗牙疼的效果会有独特的地方，中医止痛牙散就是根据中医理论体系下，经五位中药炮制而成，止痛效果很好。方解 ： \\u3000\\u3000中医认为胃腑积热，或风邪外袭经络，火邪循经上炎而发牙痛。肾主骨，齿为骨之余，肾阴不足，虚火上炎亦可引起牙痛。亦有多食甘酸之物，口齿不洁，垢秽蚀齿而作痛者。本方抗菌解毒、消肿止痛以达除牙痛之功效。方中以五味药都可温中止痛，其中丁香可补肾助阳，在止痛的同时对引起牙痛的肾阴不足有很好的补充效果。而高良姜、细辛、白芷可散寒温胃，祛除邪火。对牙痛有预防效果。\\r二、民间偏方止牙痛\\r\\u3000\\u30001.生姜具有消炎、止痛作用。牙痛时取鲜生姜一片，咬合于痛牙处，必要时可重复用之。 \\u3000\\u30002. 取大蒜捣乱，温热后敷在疼点上可以治疗牙髓炎、牙周炎和牙痛等症状。 \\u3000\\u30003. 取味精少许，直接涂敷牙痛处；或将适量味精加入开水中熔化，待冷却后反复含漱。 \\u3000\\u30004.取普通白酒100克放入茶缸里加上食盐10克；搅拌，等盐溶化之后放在炉子上烧开。含上一口在疼痛的地方，注意不要咽下去。 \\u3000\\u30005. 用蜂蜜点在牙痛的地方，过几分钟就好了，还能让你满口生香。 \\u3000\\u30006.口含芦荟：割一小片芦荟，剥除外皮，把内含粘性液体的果肉含在疼痛处，2至小时自行缓解。\\r三、牙痛食疗法\\r\\u3000\\u3000１.咸蛋蚝豉粥：咸鸭蛋2个，蚝豉（干牡蛎肉）100克，大米适量煲粥，连吃2-3天。适宜虚火上炎牙痛者食用。 \\u3000\\u3000２.蚝豉皮蛋盐渍瘦猪肉粥：蚝豉100克，皮蛋2个，盐渍瘦猪肉100克，大米适量煲粥吃。适宜阴虚牙齿肿痛、咽喉声嘶者食用。 \\u3000\\u3000３.绿豆鸡蛋糖水：绿豆100克，鸡蛋1个，冰糖适量。将绿豆捣碎，用水洗净，放锅里加水适量，煮至绿豆烂熟，把鸡蛋打入绿豆汤里，搅匀，稍凉后一次服完，连服2-3天。适宜风热牙痛、口腔红肿热痛的风热牙痛者食用。')\n",
      "---------\n",
      "1.0000000000000002\n",
      "('如何学好数学 ', '学数学需要的是会变通，灵活多变的想法加上必要的题量，如果是实在学不好，题海战术也可以考虑')\n",
      "0.9791935260830487\n",
      "('如何学好语文？ ', '多读多写多练多看看别人的文章!!')\n",
      "0.9791935260830487\n",
      "('如何学好语文？ ', '多读多写多练多看看别人的文章!!')\n",
      "---------\n",
      "Word2vec相似度运行时间： 35.244666735331215\n"
     ]
    }
   ],
   "source": [
    "from time import *\n",
    "def vector_similarity(v1,wv,wv_dict):\n",
    "    sims =  []\n",
    "    sims_dict ={}\n",
    "    for i,v2 in enumerate(wv):\n",
    "         if v1.any() != 0 and v2.any() != 0:\n",
    "                sim = np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "                #print(sim)\n",
    "                #print(v2)\n",
    "                sims.append(sim)\n",
    "                sims_dict[sim] = i\n",
    "                \n",
    "                \n",
    "    sims.sort(reverse=True)\n",
    "    #print(sims[0])\n",
    "    #print(sims_dict)\n",
    "    top_n = 3\n",
    "    for i in range(top_n):\n",
    "        print (sims[i])\n",
    "        print( wv_dict[sims_dict[sims[i]]])\n",
    "\n",
    "begin_time = time()  \n",
    "\n",
    "userinput1 = '下雨怎么办？'\n",
    "userinput2 = '胃痛怎么办？'\n",
    "userinput3 = '如何学好数学？ '\n",
    "v1 = sentence_vector(userinput1)\n",
    "v2 = sentence_vector(userinput2)\n",
    "v3 = sentence_vector(userinput3)\n",
    "print('---------')\n",
    "vector_similarity(v1,wv,wv_dict)\n",
    "print('---------')\n",
    "vector_similarity(v2,wv,wv_dict)\n",
    "print('---------')\n",
    "vector_similarity(v3,wv,wv_dict)\n",
    "print('---------')\n",
    "\n",
    "end_time=time()\n",
    "run_time = (end_time-begin_time)/3\n",
    "print ('Word2vec相似度运行时间：',run_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialog():\n",
    "    greeting_list1 = ['你好','您好']\n",
    "    greeting_list2 = ['你好，我是百科问答机器人小涛，有什么可以帮到您的吗？(*￣︶￣)','您好，我是百科问答机器人小涛，有什么可以帮到您的吗？(*￣︶￣)']#开始对话，问候意图\n",
    "   # bot_info = ['我是']\n",
    "    while True:\n",
    "            userInput = input(\">>>\")\n",
    "            userInput_list = jieba.cut(userInput)\n",
    "            userInput_word = ' '.join(userInput_list).split()\n",
    "            if  not set(userInput_word).isdisjoint(greeting_list1):\n",
    "                print(random.choice(greeting_list2))\n",
    "            elif  userInput == '你爸爸是谁':\n",
    "                  print('我爸爸是真气工程师，(*￣︶￣)')\n",
    "            elif  userInput == '你妈妈是谁':\n",
    "                  print('我妈妈是真气工程师，(*￣︶￣)')\n",
    "            elif  userInput == '你叫什么':\n",
    "                  print('我的名字是小涛，(*￣︶￣)')\n",
    "            elif  userInput == '你多少岁了':\n",
    "                  print('我今年8岁了哦,(*￣︶￣)')\n",
    "            elif  userInput == '你家在哪':\n",
    "                  print('我家在杭州哦,(*￣︶￣)')\n",
    "            #结束意图\n",
    "            elif userInput == '再见':\n",
    "                print('很高兴为您解答，欢迎再次询问,(*￣︶￣)')\n",
    "                break\n",
    "            #问问题意图还有待优化，用分类来分场景\n",
    "            elif len(userInput) != 0:\n",
    "                v = sentence_vector(userInput)\n",
    "               \n",
    "                sim,q_answer =  vector_similarity(v,wv,wv_dict)\n",
    "                if sim>=0.8:\n",
    "                    print(q_answer[1]) \n",
    "                else:\n",
    "                    print('对不起，我不懂您的问题！')\n",
    "            #无意图\n",
    "            else:\n",
    "                print(\"对不起，我不明白您讲得是什么！\")\n",
    "if __name__=='__main__':\n",
    "        dialog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from wxbot import WXBot\n",
    "import requests\n",
    "\n",
    "# base url\n",
    "bot_api=\"http://127.0.0.1:8000/get_response\"\n",
    "\n",
    "# 处理回复\n",
    "class MyWXBot(WXBot):\n",
    "    def handle_msg_all(self, msg):\n",
    "        if msg['msg_type_id'] == 4 and msg['content']['type'] == 0:\n",
    "            user_input = msg[\"content\"][\"data\"]\n",
    "            payload={\"user_input\":user_input}\n",
    "            response = requests.get(bot_api,params=payload).json()[\"response\"]\n",
    "            self.send_msg_by_uid(response, msg['user']['id'])\n",
    "\n",
    "def main():\n",
    "    bot = MyWXBot()\n",
    "    bot.DEBUG = True\n",
    "    bot.conf['qr'] = 'png'\n",
    "    bot.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1425170\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a94401b77e78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mtfv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mtfv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \"\"\"\n\u001b[1;32m-> 1361\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pickle\n",
    "import operator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder as LE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import nltk\n",
    "import jieba\n",
    "\n",
    "\n",
    "\n",
    "def cleanup(datapath):\n",
    "    with open(datapath, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()\n",
    "                    words = []\n",
    "                    labels= [] \n",
    "                    print(len(lines))\n",
    "                    for line in lines:\n",
    "                        line = json.loads(line)\n",
    "                        label  = line['category']\n",
    "                        word = line['title'] \n",
    "                        w_list = jieba.cut(word)\n",
    "                        words.append(' '.join(w_list).split())\n",
    "                        labels.append(label)\n",
    "                    return words,labels    \n",
    "\n",
    "datapath = 'E:/litao/Chatbots/data/baike_qa/baike_qa_train.json'\n",
    "words,labels = cleanup(datapath)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "le = LE()\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=1)\n",
    "tfv.fit(X)\n",
    "le.fit(Y)\n",
    "\n",
    "X = tfv.transform(X)\n",
    "y = le.transform(Y)\n",
    "\"\"\"\n",
    "\n",
    "trainx, testx, trainy, testy = tts(X, y, test_size=.25, random_state=42)\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(trainx, trainy)\n",
    "print(\"SVC:\", model.score(testx, testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 17:22:03,028 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-08-22 17:22:03,030 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "2019-08-22 17:22:03,032 : INFO : collecting document frequencies\n",
      "2019-08-22 17:22:03,033 : INFO : PROGRESS: processing document #0\n",
      "2019-08-22 17:22:03,034 : INFO : calculating IDF weights for 9 documents and 12 features (28 matrix non-zeros)\n",
      "2019-08-22 17:22:03,039 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2019-08-22 17:22:03,040 : INFO : creating matrix with 9 documents and 12 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------1----------\n",
      "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey']]\n",
      "-----------2----------\n",
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n",
      "-----------3----------\n",
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n",
      "-----------4----------\n",
      "[(0, 1), (1, 1)]\n",
      "-----------5----------\n",
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n",
      "-----------6----------\n",
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n",
      "-----------7----------\n",
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n",
      "-----------8----------\n",
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n",
      "-----------9----------\n",
      "[0.81649655 0.31412902 0.         0.3477732  0.         0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-  \n",
    "from gensim import corpora, models, similarities  \n",
    "import logging  \n",
    "from collections import defaultdict    \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)  \n",
    "  \n",
    "#文档  \n",
    "documents = [\"Human machine interface for lab abc computer applications\",  \n",
    "\"A survey of user opinion of computer system response time\",   \n",
    "\"The EPS user interface management system\",   \n",
    "\"System and human system engineering testing of EPS\",  \n",
    "\"Relation of user perceived response time to error measurement\",  \n",
    "\"The generation of random binary unordered trees\",    \n",
    "\"The intersection graph of paths in trees\",    \n",
    "\"Graph minors IV Widths of trees and well quasi ordering\",    \n",
    "\"Graph minors A survey\"]  \n",
    "  \n",
    "#1.分词，去除停用词  \n",
    "stoplist=set('for a of the and to in'.split()) \n",
    "texts=[[word for word in document.lower().split() if word not in stoplist] for document in documents]  \n",
    "print('-----------1----------')  \n",
    "print(texts)  \n",
    "#[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],  \n",
    "#['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived  \n",
    "#', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'],  \n",
    "#['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey']]  \n",
    "  \n",
    "#2.计算词频  \n",
    "frequency = defaultdict(int) #构建一个字典对象  \n",
    "#遍历分词后的结果集，计算每个词出现的频率  \n",
    "for text in texts:  \n",
    "    for token in text:  \n",
    "        frequency[token]+=1  \n",
    "#选择频率大于1的词  \n",
    "texts=[[token for token in text if frequency[token]>1] for text in texts]  \n",
    "print('-----------2----------')  \n",
    "print(texts)  \n",
    "#[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system',  \n",
    "#'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]  \n",
    "  \n",
    "#3.创建字典（单词与编号之间的映射）  \n",
    "dictionary=corpora.Dictionary(texts)  \n",
    "#print(dictionary)  \n",
    "#Dictionary(12 unique tokens: ['time', 'computer', 'graph', 'minors', 'trees']...)  \n",
    "#打印字典，key为单词，value为单词的编号  \n",
    "print('-----------3----------')  \n",
    "print(dictionary.token2id)  \n",
    "#{'human': 0, 'interface': 1, 'computer': 2, 'survey': 3, 'user': 4, 'system': 5, 'response': 6, 'time': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}  \n",
    "  \n",
    "#4.将要比较的文档转换为向量（词袋表示方法）  \n",
    "#要比较的文档  \n",
    "new_doc = \"Human computer interaction\"  \n",
    "#将文档分词并使用doc2bow方法对每个不同单词的词频进行了统计，并将单词转换为其编号，然后以稀疏向量的形式返回结果  \n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())  \n",
    "print('-----------4----------')  \n",
    "print(new_vec)  \n",
    "#[[(0, 1), (2, 1)]  \n",
    "  \n",
    "#5.建立语料库  \n",
    "#将每一篇文档转换为向量  \n",
    "corpus = [dictionary.doc2bow(text) for text in texts]  \n",
    "print('-----------5----------')  \n",
    "print(corpus)  \n",
    "#[[[(0, 1), (1, 1), (2, 1)], [(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(1, 1), (4, 1), (5, 1), (8, 1)], [(0, 1), (5, 2), (8, 1)], [(4, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(3, 1), (10, 1), (11, 1)]]  \n",
    "  \n",
    "#6.初始化模型  \n",
    "# 初始化一个tfidf模型,可以用它来转换向量（词袋整数计数）表示方法为新的表示方法（Tfidf 实数权重）  \n",
    "tfidf = models.TfidfModel(corpus)  \n",
    "#测试  \n",
    "test_doc_bow = [(0, 1), (1, 1)]  \n",
    "print('-----------6----------')  \n",
    "print(tfidf[test_doc_bow])  \n",
    "#[(0, 0.7071067811865476), (1, 0.7071067811865476)]  \n",
    "  \n",
    "print('-----------7----------')  \n",
    "#将整个语料库转为tfidf表示方法  \n",
    "corpus_tfidf = tfidf[corpus]  \n",
    "for doc in corpus_tfidf:  \n",
    "    print(doc)  \n",
    "  \n",
    "#7.创建索引  \n",
    "index = similarities.MatrixSimilarity(corpus_tfidf)  \n",
    "  \n",
    "print('-----------8----------')  \n",
    "#8.相似度计算  \n",
    "new_vec_tfidf=tfidf[new_vec]#将要比较文档转换为tfidf表示方法  \n",
    "print(new_vec_tfidf)  \n",
    "#[(0, 0.7071067811865476), (2, 0.7071067811865476)]  \n",
    "print('-----------9----------')  \n",
    "#计算要比较的文档与语料库中每篇文档的相似度  \n",
    "sims = index[new_vec_tfidf]  \n",
    "print(sims)  \n",
    "#[ 0.81649655  0.31412902  0.          0.34777319  0.          0.          0.  \n",
    "#  0.          0.        ]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 先使用对话语料训练分类模型，再使用分类模型对用户question进行分类，\n",
    "然后到具体分类模块下进行相似度的计算,最后返回相似度最高的一个或多个answer\n",
    "\"\"\"\n",
    "import jieba\n",
    "import gensim\n",
    "import json\n",
    "import  pandas as pd\n",
    "from sklearn import cross_validation\n",
    "import  numpy as np\n",
    "from sklearn import svm\n",
    "import torch\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    labels = []\n",
    "    words = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = json.loads(line)\n",
    "            question = line['title']\n",
    "            label = line['category']\n",
    "            q_list = jieba.cut(question)\n",
    "            words.append(' '.join(q_list).split())\n",
    "            labels.append(label)\n",
    "    return words,labels\n",
    "\n",
    "def word2id(words):\n",
    "    #处理words\n",
    "    all_words = [num for elem in words for num in elem]\n",
    "    #通过pd.Series将all_words里的词做成id-词\n",
    "    sr_allwords = pd.Series(all_words)\n",
    "    #print(sr_allwords)\n",
    "    #统计每个词出现的次数\n",
    "    sr_allwords = sr_allwords.value_counts()\n",
    "    #print(sr_allwords)\n",
    "    #把词单独提出来，形成词集合，去重\n",
    "    set_words = sr_allwords.index\n",
    "    #print(set_words)\n",
    "    #生成id集合\n",
    "    set_ids = range(1, len(set_words)+1)\n",
    "    #print(set_ids[:5])\n",
    "    #生成word-id表\n",
    "    word2id = pd.Series(set_ids, index=set_words)\n",
    "    #print(word2id)\n",
    "    #生成id-word表\n",
    "    id2word = pd.Series(set_words, index=set_ids)\n",
    "    #print(id2word)\n",
    "    #在word2id表的末尾加上\"BLANK\"和\"UNKNOW\"\n",
    "    word2id[\"BLANK\"]=len(word2id)+1\n",
    "    word2id[\"UNKNOW\"]=len(word2id)+1\n",
    "    #在id2word表的末尾加上\"BLANK\"和\"UNKNOW\"\n",
    "    id2word[len(id2word)+1]=\"BLANK\"\n",
    "    id2word[len(id2word)+1]=\"UNKNOW\"\n",
    "    return word2id,id2word\n",
    "#处理word，把word转成id\n",
    "#不够序列长度的，进行padding\n",
    "def X_padding(words,word2id):\n",
    "    max_len = 50  # 定义序列长度\n",
    "    \"\"\"把 words 转为 id 形式，并自动补全位 max_len 长度。\"\"\"\n",
    "    ids = []\n",
    "    #如果词在word2id表中，则返回其id,否则返回未知词\"UNKNOW\"的id\n",
    "    for i in words:\n",
    "        if i in word2id:\n",
    "            ids.append(word2id[i])\n",
    "        else:\n",
    "            ids.append(word2id[\"UNKNOW\"])\n",
    "    #如果ids的长度大于等于最大序列长度则将超出的部分截取掉，否则将ids进行扩展\n",
    "    if len(ids) >= max_len:\n",
    "        return ids[:max_len]\n",
    "    ids.extend([word2id[\"BLANK\"]]*(max_len-len(ids)))\n",
    "\n",
    "    return ids\n",
    "\n",
    "def word2vec(datas,id2word):\n",
    "    w2vecs = []\n",
    "    prevecs = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        'E:/litao/Relation_Extraction/SelfAtt_BLSTM_CH/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt',\n",
    "        binary=False, encoding='utf-8')\n",
    "    \"\"\"\n",
    "    for line in datas:\n",
    "        #line = X_padding(line)\n",
    "        for word in line:\n",
    "            if word in prevecs:\n",
    "                w2vecs.append(prevecs[word])\n",
    "    return w2vecs\n",
    "    \"\"\"\n",
    "    for i in range(len(prevecs.index2word)):\n",
    "        try:\n",
    "            index = word2id[prevecs.index2word[i]]\n",
    "            #print(\"yes\")\n",
    "        except:\n",
    "            continue\n",
    "            w2vecs[index, :] = torch.from_numpy(prevecs.get_vector(id2word[word2id[prevecs.index2word[i]]]))\n",
    "    return w2vecs\n",
    "def process_labels(labels):\n",
    "    Y = []\n",
    "    for i, label in enumerate(labels):\n",
    "      Y.append(i)\n",
    "    return Y\n",
    "\n",
    "\n",
    "#SVM多分类\n",
    "def SVM(X_train,y_train,X_test,y_test,question):\n",
    "    clf = svm.SVC(decision_function_shape='ovo')\n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "         decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',\n",
    "         max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "         tol=0.001, verbose=False)\n",
    "    print(\"Score:%.2f\"%clf.score(X_test,y_test))\n",
    "    question_list = jieba.cut(question)\n",
    "    question_words = ' '.join(question_list).split()\n",
    "    q2vec = word2vec(question_words)\n",
    "    predicted = clf.predict(q2vec)\n",
    "    return predicted\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = 'E:/litao/Chatbots/data/baike_qa/baike_qa_train.json'\n",
    "    words,labels = preprocess_data(file_path)\n",
    "    word2id, id2word = word2id(words)\n",
    "    words = X_padding(words,word2id)\n",
    "    X = word2vec(words,id2word)\n",
    "    Y = preprocess_data(labels)\n",
    "    X_train,X_test,y_train,y_test= cross_validation.train_test_split(X,Y,test_size=0.25)\n",
    "    SVM(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    #print(words)\n",
    "   # print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
